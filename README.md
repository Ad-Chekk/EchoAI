# Web Content Analyzer with LLMs ğŸ§ ğŸŒ

A powerful, intelligent web content analysis framework built with advanced **Large Language Models (LLMs)** and traditional **Machine Learning pipelines**. This tool can intelligently scrape, analyze, and visualize web data for sentiment, topics, summarization, and more.

## ğŸ” Features

- ğŸ§  LLM-powered content understanding (Summarization, NER, etc.)
- ğŸ•·ï¸ Custom web scraping (Scrapy-based)
- ğŸ“Š NLP pipelines: Sentiment analysis, POS tagging, keyword extraction
- âš¡ Streamlit dashboard for real-time visualization
- ğŸ§± Modular design with reusable ML pipelines
- ğŸ“¦ Docker-ready setup

---


## ğŸ§  LLM & ML Models Used

### ğŸ”¹ LLMs
- **HuggingFace Transformers**
  - `facebook/bart-large-cnn` â€“ Text Summarization
  - `dslim/bert-base-NER` â€“ Named Entity Recognition
  -  `eepset/roberta-base-squad2` - Q and A based system

### ğŸ”¹ Classical ML/NLP Models
- **Sentiment Analysis** â€“ Logistic Regression + TF-IDF (custom-trained)
- **Keyword Extraction** â€“ RAKE (Rapid Automatic Keyword Extraction)
- **POS Tagging** â€“ spaCyâ€™s `en_core_web_sm` pipeline

---

## âŸ³ ML Pipelines

| Pipeline | Description |
|---------|-------------|
| `text_preprocessing`|punctuation removal, stopword filtering |
| `summarization.py` | BART-based summarizer (LLM) |
| `keyword_entity`|for keyword and entity extraction
| `QandAmodule.py` | Question and answering model |
| `scraper.py` | simple scraping purpose |

---

## ğŸš€ Getting Started

### 1. Clone the Repo
```bash
git clone https://github.com/yourusername/web-content-analyzer
cd web-content-analyzer
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the UI
streamlit run app.py


## ğŸ“Š Output Examples

- Summarized text
- Extracted entities
- Sentiment polarity (positive/negative/neutral)
- Top keywords
- Visualized content charts

---


## ğŸ“œ License

MIT License â€” feel free to for